```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
``` 




Human Activity Recognition -- Weight Lifting Exercise Learning
========================================================

Data Induction
---------------------------

The Weight Lifting Exercise Dataset was collected from the Human Activity Recognition Project in Groupware@LES. (http://groupware.les.inf.puc-rio.br/har) In this study, six young health participants, who were wearing accaccelerometers on the belt, forearm, arm, and dumbell, were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions. We want to investigate "how (well)" an activity was performed by the wearer. The manner they did the exercise (the "classe" variable in the dataset) is the outcome of interest. 159 features are given to help building the prediction algorithm.

Data Cleaning
----------------------

Before performing any analysis, we did some data cleaning in the first place. The following types of feature variables are excluded:
(1) Variables with missing values
(2) Identity and timestamp features

After the data cleaning step, 52 features are left for further analysis.

Model
------------------

We use the Random Forest learning algorithm to build the model. Generally the steps are as follows. We resample the training data, then rebuild classification or regression trees on each of the resampled data. When we split the data each time in a classification tree, we also resample the variables. A large number of trees are then built. We vote or average those trees in order to get the prediction for a new outcome. Random Forest method is highly accurate in many cases. However, it may counter several problems such as low speed and overfitting.

Feature Seclection
------------------

The method we apply in this section is to pre-screen the predictors using simple univariate statistical methods then only use those that pass some criterion in the subsequent model steps. The function applied in this step can be used to get resampling estimates for models when simple, filter-based feature selection is applied to the training data.

For each iteration of resampling, the predictor variables are univariately filtered prior to modeling. Performance of this approach is estimated using resampling. The same filter and model are then applied to the entire training set and the final model (and final features) are saved. In our analysis, we use repeated 10 folds cross validation with repeated times equals 5 to select features. 

After feature selection step, 46 variables are chosen in further analysis.

```{r include=F}
load("rfWithfilter.RData")
load("profile.RData")
library(lattice)
library(randomForest)
load('test_clean.rdata')
```
```{r}
rfWithFilter$optVariables
densityplot(rfWithFilter)
```

Cross Validation
---------------------

With regard to the potential overfitting problem of Random Forest, we perform a 10-fold cross validation to perform recursive feature selection. We use 10, 20, 30 and 40 as the cadidate subset sizes. The CV procedure is as follows:

1. Split the dataset into 10 dataset.  
2. Each time, 9/10 datasets are training data, 1/10 is testing. Do:  
   *Train the model on the training set using all predictors then predict the testing set  
   *Calculate the variable importance or rankings.  
   *For each subset size S(i), keep the S(i) most important variables, train the model on the training set using S(i) predictors, predict the testing set.  
3. Calculate the performance profile over the S(i) using test samples, determine the proper number of predictors.  
4. Estimate the final list of predictors to keep in the final model.  
5. Fit the final model based on the optimal S(i) using the original training set.

Here are the codes
```{r,eval= F}
profile <- rfe(train_sel, train_clean$classe,
               sizes = c(10,20,30,40),
               rfeControl = rfeControl(functions=rfFuncs,method="cv"))
```
  
Result
---------------------------

The output and first figure below show that the best subset size was estimated to be 40 predictors. The density plot showed the prediction accuracy for these variables in 10 cross validation iterations. The prediction accuracy based on cross validation result on the 40 predictors selected is 0.997. The out-of-sample error is 0.003415.

```{r include=F}
load("rfWithfilter.RData")
load("profile.RData")
library(lattice)
library(randomForest)
load('test_clean.rdata')
```

```{r comment=NA}

print(profile)
(error <- (1 - subset(profile$results, Variables==40)$Accuracy))
plot(profile, type = c("g", "o"))
densityplot(profile)
```

The optimal 40 predictors are listed as follows:
```{r comment=NA}
profile$optVariables
```

Prediction
---------------------------------

We will use the bulit algorithm to predict the 20 test samples. Here is the result.

```{r include=F}
load("rfWithfilter.RData")
load("profile.RData")
library(lattice)
library(randomForest)
load('test_clean.rdata')
```
```{r comment=NA}
predict(profile$fit,test_clean)
```
